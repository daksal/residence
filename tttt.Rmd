---
title: "야놀자 & 여기어때"
author: "언론홍보학과 정윤혁"
date: "21/09/2022"
format: html
editor: visual
code-fold: true
code-tools: true
editor_options: 
  chunk_output_type: console
---

# 야놀자 & 여기 어때

-   기간: 2022.01 \~ 2022.09.18

빅카인즈 키워드 검색을 통해 '야놀자'와 '여기 어때' 기업에 대한 기사를 분석함

분석 기간은 2022년 01월부터 09월까지의 기사를 다루었다.

기사의 갯수로 야놀자는 754건, 여기 어때는 156건을 분석했다.

분석 순서로는 **총빈도, 상대빈도, 감정어 분석, 주제모형** 순으로 분석했다.

### 1. 분석에 앞선 기본적인 준비

##### *1) 패키지 설치 및 불러오기*

```{r, label: 패키지 불러오기, include: false, warning: false}
 
c( "rio", "psych", "psychTools", "skimr", "janitor", 

  "tidyverse", "tidytable", "tidymodels", "lm.beta",

  "GGally", "ggforce", "gt", "mdthemes", "patchwork",

  "r2symbols", "equatiomatic", "purrr", "palmerpenguins",

  "textdata", "tidytext", "epubr", "stm", "quarto", "wordcloud",

  "RcppMeCab", "KoNLP", "tidyr", "dplyr", "tidylo", "lubridate"

) -> pkg 

lapply(pkg, require, ch = T)

sapply(pkg, function(x){

  if(!require(x, ch = T)) install.packages(x)

  library(x, ch = T)

})
```

##### *2) 파일 경로 설정*

```{r}
#| label: 파일경로 설정
#| include: false
#| warning: false

setwd("C:/Users/YH JUNG/Documents/R/GIT/residence/data")

getwd()
```

##### *3) 감정어 사전 설치 및 불러오기*

```{r}
#| label: 감정어 사전 다운 및 설치
#| include: false
#| warning: false

url_v <- "https://github.com/park1200656/KnuSentiLex/archive/refs/heads/master.zip"

dest_v <- "data/knusenti.zip"

download.file(url = url_v, destfile = dest_v, mode = "wb")

list.files("data/KnuSentiLex-master/")
```

##### *4) 자료 불러오기 및 자료 정제*

```{r}
#| label: 빅카인즈 야놀자 & 여기어때 자료
#| include: false
#| warning: false

# 야놀자
yanolja_df <- readxl::read_excel("C:/Users/YH JUNG/Documents/R/GIT/residence/data/yanolja_after_covid.xlsx") %>%
  
select.(제목, 언론사, 본문, URL)

yanolja_tk <- yanolja_df %>%
  
drop_na() %>%
  
unnest_tokens(word, 제목, token = pos) %>%
  
separate(word, c("word", "pos"), sep = "/") %>%
  
filter(pos == 'nng')

#여기어때
hah_df <- readxl::read_excel("C:/Users/YH JUNG/Documents/R/GIT/residence/data/hah_after_covid.xlsx") %>%
  
select.(제목, 언론사, 본문, URL)

hah_tk <- hah_df %>%
  
drop_na() %>%
  
unnest_tokens(word, 제목, token = pos) %>%
  
separate(word, c("word", "pos"), sep = "/") %>%
  
filter(pos == 'nng')
```



## 
